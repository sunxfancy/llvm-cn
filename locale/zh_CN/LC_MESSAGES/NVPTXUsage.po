# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2003-2016, LLVM Project
# This file is distributed under the same license as the LLVM package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2016.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: LLVM 3.8\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2016-04-17 11:36+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.3.3\n"

#: ../../NVPTXUsage.rst:3
msgid "User Guide for NVPTX Back-end"
msgstr ""

#: ../../NVPTXUsage.rst:11
msgid "Introduction"
msgstr ""

#: ../../NVPTXUsage.rst:13
msgid ""
"To support GPU programming, the NVPTX back-end supports a subset of LLVM "
"IR along with a defined set of conventions used to represent GPU "
"programming concepts. This document provides an overview of the general "
"usage of the back- end, including a description of the conventions used "
"and the set of accepted LLVM IR."
msgstr ""

#: ../../NVPTXUsage.rst:21
msgid ""
"This document assumes a basic familiarity with CUDA and the PTX assembly "
"language. Information about the CUDA Driver API and the PTX assembly "
"language can be found in the `CUDA documentation "
"<http://docs.nvidia.com/cuda/index.html>`_."
msgstr ""

#: ../../NVPTXUsage.rst:29
msgid "Conventions"
msgstr ""

#: ../../NVPTXUsage.rst:32
msgid "Marking Functions as Kernels"
msgstr ""

#: ../../NVPTXUsage.rst:34
msgid ""
"In PTX, there are two types of functions: *device functions*, which are "
"only callable by device code, and *kernel functions*, which are callable "
"by host code. By default, the back-end will emit device functions. "
"Metadata is used to declare a function as a kernel function. This "
"metadata is attached to the ``nvvm.annotations`` named metadata object, "
"and has the following format:"
msgstr ""

#: ../../NVPTXUsage.rst:44
msgid ""
"The first parameter is a reference to the kernel function. The following "
"example shows a kernel function calling a device function in LLVM IR. The"
" function ``@my_kernel`` is callable from host code, but ``@my_fmad`` is "
"not."
msgstr ""

#: ../../NVPTXUsage.rst:66
msgid "When compiled, the PTX kernel functions are callable by host-side code."
msgstr ""

#: ../../NVPTXUsage.rst:72 ../../NVPTXUsage.rst:574
msgid "Address Spaces"
msgstr ""

#: ../../NVPTXUsage.rst:74
msgid "The NVPTX back-end uses the following address space mapping:"
msgstr ""

#: ../../NVPTXUsage.rst:77
msgid "Address Space"
msgstr ""

#: ../../NVPTXUsage.rst:77
msgid "Memory Space"
msgstr ""

#: ../../NVPTXUsage.rst:79
msgid "0"
msgstr ""

#: ../../NVPTXUsage.rst:79
msgid "Generic"
msgstr ""

#: ../../NVPTXUsage.rst:80
msgid "1"
msgstr ""

#: ../../NVPTXUsage.rst:80
msgid "Global"
msgstr ""

#: ../../NVPTXUsage.rst:81
msgid "2"
msgstr ""

#: ../../NVPTXUsage.rst:81
msgid "Internal Use"
msgstr ""

#: ../../NVPTXUsage.rst:82
msgid "3"
msgstr ""

#: ../../NVPTXUsage.rst:82
msgid "Shared"
msgstr ""

#: ../../NVPTXUsage.rst:83
msgid "4"
msgstr ""

#: ../../NVPTXUsage.rst:83
msgid "Constant"
msgstr ""

#: ../../NVPTXUsage.rst:84
msgid "5"
msgstr ""

#: ../../NVPTXUsage.rst:84
msgid "Local"
msgstr ""

#: ../../NVPTXUsage.rst:87
msgid ""
"Every global variable and pointer type is assigned to one of these "
"address spaces, with 0 being the default address space. Intrinsics are "
"provided which can be used to convert pointers between the generic and "
"non-generic address spaces."
msgstr ""

#: ../../NVPTXUsage.rst:92
msgid ""
"As an example, the following IR will define an array ``@g`` that resides "
"in global device memory."
msgstr ""

#: ../../NVPTXUsage.rst:99
msgid ""
"LLVM IR functions can read and write to this array, and host-side code "
"can copy data to it by name with the CUDA Driver API."
msgstr ""

#: ../../NVPTXUsage.rst:102
msgid ""
"Note that since address space 0 is the generic space, it is illegal to "
"have global variables in address space 0.  Address space 0 is the default"
" address space in LLVM, so the ``addrspace(N)`` annotation is *required* "
"for global variables."
msgstr ""

#: ../../NVPTXUsage.rst:109
msgid "Triples"
msgstr ""

#: ../../NVPTXUsage.rst:111
msgid ""
"The NVPTX target uses the module triple to select between 32/64-bit code "
"generation and the driver-compiler interface to use. The triple "
"architecture can be one of ``nvptx`` (32-bit PTX) or ``nvptx64`` (64-bit "
"PTX). The operating system should be one of ``cuda`` or ``nvcl``, which "
"determines the interface used by the generated code to communicate with "
"the driver.  Most users will want to use ``cuda`` as the operating "
"system, which makes the generated PTX compatible with the CUDA Driver "
"API."
msgstr ""

#: ../../NVPTXUsage.rst:119
msgid "Example: 32-bit PTX for CUDA Driver API: ``nvptx-nvidia-cuda``"
msgstr ""

#: ../../NVPTXUsage.rst:121
msgid "Example: 64-bit PTX for CUDA Driver API: ``nvptx64-nvidia-cuda``"
msgstr ""

#: ../../NVPTXUsage.rst:128
msgid "NVPTX Intrinsics"
msgstr ""

#: ../../NVPTXUsage.rst:131
msgid "Address Space Conversion"
msgstr ""

#: ../../NVPTXUsage.rst:134
msgid "'``llvm.nvvm.ptr.*.to.gen``' Intrinsics"
msgstr ""

#: ../../NVPTXUsage.rst:137 ../../NVPTXUsage.rst:165 ../../NVPTXUsage.rst:200
#: ../../NVPTXUsage.rst:242
msgid "Syntax:"
msgstr ""

#: ../../NVPTXUsage.rst:139 ../../NVPTXUsage.rst:167
msgid "These are overloaded intrinsics.  You can use these on any pointer types."
msgstr ""

#: ../../NVPTXUsage.rst:149 ../../NVPTXUsage.rst:177 ../../NVPTXUsage.rst:219
#: ../../NVPTXUsage.rst:249
msgid "Overview:"
msgstr ""

#: ../../NVPTXUsage.rst:151
msgid ""
"The '``llvm.nvvm.ptr.*.to.gen``' intrinsics convert a pointer in a non-"
"generic address space to a generic address space pointer."
msgstr ""

#: ../../NVPTXUsage.rst:155 ../../NVPTXUsage.rst:187
msgid "Semantics:"
msgstr ""

#: ../../NVPTXUsage.rst:157
msgid ""
"These intrinsics modify the pointer value to be a valid generic address "
"space pointer."
msgstr ""

#: ../../NVPTXUsage.rst:162
msgid "'``llvm.nvvm.ptr.gen.to.*``' Intrinsics"
msgstr ""

#: ../../NVPTXUsage.rst:179
msgid ""
"The '``llvm.nvvm.ptr.gen.to.*``' intrinsics convert a pointer in the "
"generic address space to a pointer in the target address space.  Note "
"that these intrinsics are only useful if the address space of the target "
"address space of the pointer is known.  It is not legal to use address "
"space conversion intrinsics to convert a pointer from one non-generic "
"address space to another non-generic address space."
msgstr ""

#: ../../NVPTXUsage.rst:189
msgid ""
"These intrinsics modify the pointer value to be a valid pointer in the "
"target non-generic address space."
msgstr ""

#: ../../NVPTXUsage.rst:194
msgid "Reading PTX Special Registers"
msgstr ""

#: ../../NVPTXUsage.rst:197
msgid "'``llvm.nvvm.read.ptx.sreg.*``'"
msgstr ""

#: ../../NVPTXUsage.rst:221
msgid ""
"The '``@llvm.nvvm.read.ptx.sreg.*``' intrinsics provide access to the PTX"
" special registers, in particular the kernel launch bounds.  These "
"registers map in the following way to CUDA builtins:"
msgstr ""

#: ../../NVPTXUsage.rst:226
msgid "CUDA Builtin"
msgstr ""

#: ../../NVPTXUsage.rst:226
msgid "PTX Special Register Intrinsic"
msgstr ""

#: ../../NVPTXUsage.rst:228
msgid "``threadId``"
msgstr ""

#: ../../NVPTXUsage.rst:228
msgid "``@llvm.nvvm.read.ptx.sreg.tid.*``"
msgstr ""

#: ../../NVPTXUsage.rst:229
msgid "``blockIdx``"
msgstr ""

#: ../../NVPTXUsage.rst:229
msgid "``@llvm.nvvm.read.ptx.sreg.ctaid.*``"
msgstr ""

#: ../../NVPTXUsage.rst:230
msgid "``blockDim``"
msgstr ""

#: ../../NVPTXUsage.rst:230
msgid "``@llvm.nvvm.read.ptx.sreg.ntid.*``"
msgstr ""

#: ../../NVPTXUsage.rst:231
msgid "``gridDim``"
msgstr ""

#: ../../NVPTXUsage.rst:231
msgid "``@llvm.nvvm.read.ptx.sreg.nctaid.*``"
msgstr ""

#: ../../NVPTXUsage.rst:236
msgid "Barriers"
msgstr ""

#: ../../NVPTXUsage.rst:239
msgid "'``llvm.nvvm.barrier0``'"
msgstr ""

#: ../../NVPTXUsage.rst:251
msgid ""
"The '``@llvm.nvvm.barrier0()``' intrinsic emits a PTX ``bar.sync 0`` "
"instruction, equivalent to the ``__syncthreads()`` call in CUDA."
msgstr ""

#: ../../NVPTXUsage.rst:256
msgid "Other Intrinsics"
msgstr ""

#: ../../NVPTXUsage.rst:258
msgid ""
"For the full set of NVPTX intrinsics, please see the "
"``include/llvm/IR/IntrinsicsNVVM.td`` file in the LLVM source tree."
msgstr ""

#: ../../NVPTXUsage.rst:265
msgid "Linking with Libdevice"
msgstr ""

#: ../../NVPTXUsage.rst:267
msgid ""
"The CUDA Toolkit comes with an LLVM bitcode library called ``libdevice`` "
"that implements many common mathematical functions. This library can be "
"used as a high-performance math library for any compilers using the LLVM "
"NVPTX target. The library can be found under ``nvvm/libdevice/`` in the "
"CUDA Toolkit and there is a separate version for each compute "
"architecture."
msgstr ""

#: ../../NVPTXUsage.rst:273
msgid ""
"For a list of all math functions implemented in libdevice, see `libdevice"
" Users Guide <http://docs.nvidia.com/cuda/libdevice-users-"
"guide/index.html>`_."
msgstr ""

#: ../../NVPTXUsage.rst:276
msgid ""
"To accommodate various math-related compiler flags that can affect code "
"generation of libdevice code, the library code depends on a special LLVM "
"IR pass (``NVVMReflect``) to handle conditional compilation within LLVM "
"IR. This pass looks for calls to the ``@__nvvm_reflect`` function and "
"replaces them with constants based on the defined reflection parameters. "
"Such conditional code often follows a pattern:"
msgstr ""

#: ../../NVPTXUsage.rst:292
msgid "The default value for all unspecified reflection parameters is zero."
msgstr ""

#: ../../NVPTXUsage.rst:294
msgid ""
"The ``NVVMReflect`` pass should be executed early in the optimization "
"pipeline, immediately after the link stage. The ``internalize`` pass is "
"also recommended to remove unused math functions from the resulting PTX. "
"For an input IR module ``module.bc``, the following compilation flow is "
"recommended:"
msgstr ""

#: ../../NVPTXUsage.rst:299
msgid "Save list of external functions in ``module.bc``"
msgstr ""

#: ../../NVPTXUsage.rst:300
msgid "Link ``module.bc`` with ``libdevice.compute_XX.YY.bc``"
msgstr ""

#: ../../NVPTXUsage.rst:301
msgid "Internalize all functions not in list from (1)"
msgstr ""

#: ../../NVPTXUsage.rst:302
msgid "Eliminate all unused internal functions"
msgstr ""

#: ../../NVPTXUsage.rst:303
msgid "Run ``NVVMReflect`` pass"
msgstr ""

#: ../../NVPTXUsage.rst:304
msgid "Run standard optimization pipeline"
msgstr ""

#: ../../NVPTXUsage.rst:308
msgid ""
"``linkonce`` and ``linkonce_odr`` linkage types are not suitable for the "
"libdevice functions. It is possible to link two IR modules that have been"
" linked against libdevice using different reflection variables."
msgstr ""

#: ../../NVPTXUsage.rst:312
msgid ""
"Since the ``NVVMReflect`` pass replaces conditionals with constants, it "
"will often leave behind dead code of the form:"
msgstr ""

#: ../../NVPTXUsage.rst:326
msgid ""
"Therefore, it is recommended that ``NVVMReflect`` is executed early in "
"the optimization pipeline before dead-code elimination."
msgstr ""

#: ../../NVPTXUsage.rst:331
msgid "Reflection Parameters"
msgstr ""

#: ../../NVPTXUsage.rst:333
msgid ""
"The libdevice library currently uses the following reflection parameters "
"to control code generation:"
msgstr ""

#: ../../NVPTXUsage.rst:337
msgid "Flag"
msgstr ""

#: ../../NVPTXUsage.rst:337
msgid "Description"
msgstr ""

#: ../../NVPTXUsage.rst:339
msgid "``__CUDA_FTZ=[0,1]``"
msgstr ""

#: ../../NVPTXUsage.rst:339
msgid "Use optimized code paths that flush subnormals to zero"
msgstr ""

#: ../../NVPTXUsage.rst:344
msgid "Invoking NVVMReflect"
msgstr ""

#: ../../NVPTXUsage.rst:346
msgid ""
"To ensure that all dead code caused by the reflection pass is eliminated,"
" it is recommended that the reflection pass is executed early in the LLVM"
" IR optimization pipeline. The pass takes an optional mapping of "
"reflection parameter name to an integer value. This mapping can be "
"specified as either a command-line option to ``opt`` or as an LLVM "
"``StringMap<int>`` object when programmatically creating a pass pipeline."
msgstr ""

#: ../../NVPTXUsage.rst:353
msgid "With ``opt``:"
msgstr ""

#: ../../NVPTXUsage.rst:360
msgid "With programmatic pass pipeline:"
msgstr ""

#: ../../NVPTXUsage.rst:373
msgid "Executing PTX"
msgstr ""

#: ../../NVPTXUsage.rst:375
msgid ""
"The most common way to execute PTX assembly on a GPU device is to use the"
" CUDA Driver API. This API is a low-level interface to the GPU driver and"
" allows for JIT compilation of PTX code to native GPU machine code."
msgstr ""

#: ../../NVPTXUsage.rst:379
msgid "Initializing the Driver API:"
msgstr ""

#: ../../NVPTXUsage.rst:393
msgid "JIT compiling a PTX string to a device binary:"
msgstr ""

#: ../../NVPTXUsage.rst:406
msgid ""
"For full examples of executing PTX assembly, please see the `CUDA Samples"
" <https://developer.nvidia.com/cuda-downloads>`_ distribution."
msgstr ""

#: ../../NVPTXUsage.rst:411
msgid "Common Issues"
msgstr ""

#: ../../NVPTXUsage.rst:414
msgid "ptxas complains of undefined function: __nvvm_reflect"
msgstr ""

#: ../../NVPTXUsage.rst:416
msgid ""
"When linking with libdevice, the ``NVVMReflect`` pass must be used. See "
":ref:`libdevice` for more information."
msgstr ""

#: ../../NVPTXUsage.rst:421
msgid "Tutorial: A Simple Compute Kernel"
msgstr ""

#: ../../NVPTXUsage.rst:423
msgid ""
"To start, let us take a look at a simple compute kernel written directly "
"in LLVM IR. The kernel implements vector addition, where each thread "
"computes one element of the output vector C from the input vectors A and "
"B.  To make this easier, we also assume that only a single CTA (thread "
"block) will be launched, and that it will be one dimensional."
msgstr ""

#: ../../NVPTXUsage.rst:431
msgid "The Kernel"
msgstr ""

#: ../../NVPTXUsage.rst:472
msgid "We can use the LLVM ``llc`` tool to directly run the NVPTX code generator:"
msgstr ""

#: ../../NVPTXUsage.rst:481
msgid ""
"If you want to generate 32-bit code, change ``p:64:64:64`` to "
"``p:32:32:32`` in the module data layout string and use ``nvptx-nvidia-"
"cuda`` as the target triple."
msgstr ""

#: ../../NVPTXUsage.rst:486
msgid "The output we get from ``llc`` (as of LLVM 3.4):"
msgstr ""

#: ../../NVPTXUsage.rst:528
msgid "Dissecting the Kernel"
msgstr ""

#: ../../NVPTXUsage.rst:530
msgid "Now let us dissect the LLVM IR that makes up this kernel."
msgstr ""

#: ../../NVPTXUsage.rst:533
msgid "Data Layout"
msgstr ""

#: ../../NVPTXUsage.rst:535
msgid ""
"The data layout string determines the size in bits of common data types, "
"their ABI alignment, and their storage size.  For NVPTX, you should use "
"one of the following:"
msgstr ""

#: ../../NVPTXUsage.rst:539
msgid "32-bit PTX:"
msgstr ""

#: ../../NVPTXUsage.rst:545
msgid "64-bit PTX:"
msgstr ""

#: ../../NVPTXUsage.rst:553
msgid "Target Intrinsics"
msgstr ""

#: ../../NVPTXUsage.rst:555
msgid ""
"In this example, we use the ``@llvm.nvvm.read.ptx.sreg.tid.x`` intrinsic "
"to read the X component of the current thread's ID, which corresponds to "
"a read of register ``%tid.x`` in PTX. The NVPTX back-end supports a large"
" set of intrinsics.  A short list is shown below; please see "
"``include/llvm/IR/IntrinsicsNVVM.td`` for the full list."
msgstr ""

#: ../../NVPTXUsage.rst:563
msgid "Intrinsic"
msgstr ""

#: ../../NVPTXUsage.rst:563
msgid "CUDA Equivalent"
msgstr ""

#: ../../NVPTXUsage.rst:565
msgid "``i32 @llvm.nvvm.read.ptx.sreg.tid.{x,y,z}``"
msgstr ""

#: ../../NVPTXUsage.rst:565
msgid "threadIdx.{x,y,z}"
msgstr ""

#: ../../NVPTXUsage.rst:566
msgid "``i32 @llvm.nvvm.read.ptx.sreg.ctaid.{x,y,z}``"
msgstr ""

#: ../../NVPTXUsage.rst:566
msgid "blockIdx.{x,y,z}"
msgstr ""

#: ../../NVPTXUsage.rst:567
msgid "``i32 @llvm.nvvm.read.ptx.sreg.ntid.{x,y,z}``"
msgstr ""

#: ../../NVPTXUsage.rst:567
msgid "blockDim.{x,y,z}"
msgstr ""

#: ../../NVPTXUsage.rst:568
msgid "``i32 @llvm.nvvm.read.ptx.sreg.nctaid.{x,y,z}``"
msgstr ""

#: ../../NVPTXUsage.rst:568
msgid "gridDim.{x,y,z}"
msgstr ""

#: ../../NVPTXUsage.rst:569
msgid "``void @llvm.cuda.syncthreads()``"
msgstr ""

#: ../../NVPTXUsage.rst:569
msgid "__syncthreads()"
msgstr ""

#: ../../NVPTXUsage.rst:576
msgid ""
"You may have noticed that all of the pointer types in the LLVM IR example"
" had an explicit address space specifier. What is address space 1? NVIDIA"
" GPU devices (generally) have four types of memory:"
msgstr ""

#: ../../NVPTXUsage.rst:580
msgid "Global: Large, off-chip memory"
msgstr ""

#: ../../NVPTXUsage.rst:581
msgid "Shared: Small, on-chip memory shared among all threads in a CTA"
msgstr ""

#: ../../NVPTXUsage.rst:582
msgid "Local: Per-thread, private memory"
msgstr ""

#: ../../NVPTXUsage.rst:583
msgid "Constant: Read-only memory shared across all threads"
msgstr ""

#: ../../NVPTXUsage.rst:585
msgid ""
"These different types of memory are represented in LLVM IR as address "
"spaces. There is also a fifth address space used by the NVPTX code "
"generator that corresponds to the \"generic\" address space.  This "
"address space can represent addresses in any other address space (with a "
"few exceptions).  This allows users to write IR functions that can "
"load/store memory using the same instructions. Intrinsics are provided to"
" convert pointers between the generic and non-generic address spaces."
msgstr ""

#: ../../NVPTXUsage.rst:593
msgid ""
"See :ref:`address_spaces` and :ref:`nvptx_intrinsics` for more "
"information."
msgstr ""

#: ../../NVPTXUsage.rst:597
msgid "Kernel Metadata"
msgstr ""

#: ../../NVPTXUsage.rst:599
msgid ""
"In PTX, a function can be either a `kernel` function (callable from the "
"host program), or a `device` function (callable only from GPU code). You "
"can think of `kernel` functions as entry-points in the GPU program. To "
"mark an LLVM IR function as a `kernel` function, we make use of special "
"LLVM metadata. The NVPTX back-end will look for a named metadata node "
"called ``nvvm.annotations``. This named metadata must contain a list of "
"metadata that describe the IR. For our purposes, we need to declare a "
"metadata node that assigns the \"kernel\" attribute to the LLVM IR "
"function that should be emitted as a PTX `kernel` function. These "
"metadata nodes take the form:"
msgstr ""

#: ../../NVPTXUsage.rst:613
msgid "For the previous example, we have:"
msgstr ""

#: ../../NVPTXUsage.rst:622
msgid ""
"Here, we have a single metadata declaration in ``nvvm.annotations``. This"
" metadata annotates our ``@kernel`` function with the ``kernel`` "
"attribute."
msgstr ""

#: ../../NVPTXUsage.rst:627
msgid "Running the Kernel"
msgstr ""

#: ../../NVPTXUsage.rst:629
msgid ""
"Generating PTX from LLVM IR is all well and good, but how do we execute "
"it on a real GPU device? The CUDA Driver API provides a convenient "
"mechanism for loading and JIT compiling PTX to a native GPU device, and "
"launching a kernel. The API is similar to OpenCL.  A simple example "
"showing how to load and execute our vector addition code is shown below. "
"Note that for brevity this code does not perform much error checking!"
msgstr ""

#: ../../NVPTXUsage.rst:638
msgid ""
"You can also use the ``ptxas`` tool provided by the CUDA Toolkit to "
"offline compile PTX to machine code (SASS) for a specific GPU "
"architecture. Such binaries can be loaded by the CUDA Driver API in the "
"same way as PTX. This can be useful for reducing startup time by "
"precompiling the PTX kernels."
msgstr ""

#: ../../NVPTXUsage.rst:767
msgid "You will need to link with the CUDA driver and specify the path to cuda.h."
msgstr ""

#: ../../NVPTXUsage.rst:773
msgid ""
"We don't need to specify a path to ``libcuda.so`` since this is installed"
" in a system location by the driver, not the CUDA toolkit."
msgstr ""

#: ../../NVPTXUsage.rst:776
msgid ""
"If everything goes as planned, you should see the following output when "
"running the compiled program:"
msgstr ""

#: ../../NVPTXUsage.rst:804
msgid "You will likely see a different device identifier based on your hardware"
msgstr ""

#: ../../NVPTXUsage.rst:808
msgid "Tutorial: Linking with Libdevice"
msgstr ""

#: ../../NVPTXUsage.rst:810
msgid ""
"In this tutorial, we show a simple example of linking LLVM IR with the "
"libdevice library. We will use the same kernel as the previous tutorial, "
"except that we will compute ``C = pow(A, B)`` instead of ``C = A + B``. "
"Libdevice provides an ``__nv_powf`` function that we will use."
msgstr ""

#: ../../NVPTXUsage.rst:856
msgid "To compile this kernel, we perform the following steps:"
msgstr ""

#: ../../NVPTXUsage.rst:858
msgid "Link with libdevice"
msgstr ""

#: ../../NVPTXUsage.rst:859
msgid "Internalize all but the public kernel function"
msgstr ""

#: ../../NVPTXUsage.rst:860
msgid "Run ``NVVMReflect`` and set ``__CUDA_FTZ`` to 0"
msgstr ""

#: ../../NVPTXUsage.rst:861
msgid "Optimize the linked module"
msgstr ""

#: ../../NVPTXUsage.rst:862
msgid "Codegen the module"
msgstr ""

#: ../../NVPTXUsage.rst:865
msgid ""
"These steps can be performed by the LLVM ``llvm-link``, ``opt``, and "
"``llc`` tools. In a complete compiler, these steps can also be performed "
"entirely programmatically by setting up an appropriate pass configuration"
" (see :ref:`libdevice`)."
msgstr ""

#: ../../NVPTXUsage.rst:878
msgid ""
"The ``-nvvm-reflect-list=_CUDA_FTZ=0`` is not strictly required, as any "
"undefined variables will default to zero. It is shown here for evaluation"
" purposes."
msgstr ""

#: ../../NVPTXUsage.rst:883
msgid "This gives us the following PTX (excerpt):"
msgstr ""

